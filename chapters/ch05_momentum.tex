\chapter{无约束优化之动量}

\section{符号说明}

\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{符号} & \textbf{含义} \\ \hline
$x_t$ & 第$t$步的参数向量 \\ \hline
$\nabla f(x_t)$ & 目标函数在$x_t$处的梯度 \\ \hline
$\eta$ & 学习率（步长） \\ \hline
$\beta, \beta_1, \beta_2$ & 动量/滑动平均系数（通常取 0.9, 0.999 等） \\ \hline
$v_t$ & 第$t$步的动量项（速度） \\ \hline
$G_{t,i}$ & 第$i$个参数到第$t$步的历史梯度平方和（AdaGrad） \\ \hline
$E[g^2]_t$ & 梯度平方的指数滑动平均（RMSProp） \\ \hline
$m_t$ & 一阶动量（梯度的指数滑动平均，Adam） \\ \hline
$v_t$ & 二阶动量（梯度平方的指数滑动平均，Adam） \\ \hline
$\hat{m}_t, \hat{v}_t$ & 偏置校正后的一阶/二阶动量（Adam） \\ \hline
$\varepsilon$ & 数值稳定项（防止除以零，通常取$10^{-8}$） \\ \hline
$\lambda$ & 权重衰减系数（正则化强度） \\ \hline
$\nabla_{i} f_t(x_t)$ & 目标函数在$x_t$处关于第$i$个参数的偏导数 \\ \hline
$\text{diag}(G_t)$ & 以$G_t$为对角线元素的对角矩阵 \\ \hline
$I$ & 单位矩阵 \\ \hline
\end{tabular}
\caption{符号说明}
\end{table}

\section{SGD 的缺点及动量方法改进}

\subsection{问题出发点：SGD 的局限}

标准随机梯度下降（SGD）更新：
\[
x_{t+1} = x_t - \eta \nabla f(x_t)
\]

\textbf{问题：}
\begin{itemize}
    \item 在狭长峡谷形损失面（即特征方向尺度差异大）中，梯度方向不断剧烈摆动；
    \item 沿陡峭方向振荡，沿平缓方向进展缓慢；
    \item 收敛速率接近 $O(1/t)$，远慢于二阶方法。
\end{itemize}

\subsection{引入动量的核心思想：惯性}

\textbf{想法：} 像物理中有质量的粒子那样，让优化“带惯性”。动量项记为速度 $v_t$，模拟动能积累。
\[
v_{t+1} = \beta v_t + (1-\beta)(-\nabla f(x_t))
\]
\[
x_{t+1} = x_t + \eta v_{t+1}
\]

这就是 \textbf{Polyak’s Heavy Ball (1964)}。

\subsection{Heavy-Ball (Polyak Momentum)}

\begin{algorithm}[Heavy-Ball 更新规则]
\textbf{形式：}
\[
x_{t+1} = x_t - \eta \nabla f(x_t) + \beta (x_t - x_{t-1})
\]
\end{algorithm}

\textbf{解释：}
\begin{itemize}
    \item 当前步沿梯度下降；
    \item 再加上前一步的“惯性”；
    \item 像滚动的重球在势场中前进，惯性帮助越过小坑和振荡区。
\end{itemize}

\textbf{优点：}
\begin{itemize}
    \item 加速收敛；
    \item 缓解振荡。
\end{itemize}

\textbf{缺点：}
\begin{itemize}
    \item 对非凸问题容易过冲；
    \item 需要精心调节 $\beta$ 与 $\eta$。
\end{itemize}

\subsection{Nesterov 加速梯度 (NAG, 1983)}

Nesterov 注意到 Heavy-Ball 更新\textbf{滞后}：你先算完梯度，再加惯性，但惯性早就改变了位置。他提出：\textbf{提前感知未来位置}。

\begin{algorithm}[Nesterov 加速梯度 (NAG)]
\[
v_{t+1} = \beta v_t - \eta \nabla f(x_t + \beta v_t)
\]
\[
x_{t+1} = x_t + v_{t+1}
\]
\end{algorithm}

\textbf{解释：}
\begin{itemize}
    \item 先“预测”下一个位置；
    \item 在预测点计算梯度；
    \item 因此能提前修正方向。
\end{itemize}

\textbf{直觉：}
Heavy-Ball 是“被动加速”，Nesterov 是“前瞻修正”。

\subsection{对比总结}

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{特性} & \textbf{Heavy-Ball} & \textbf{Nesterov} \\ \hline
物理意义 & 惯性滚动 & 预判修正 \\ \hline
梯度计算点 & 当前点 & 预测点 \\ \hline
稳定性 & 易过冲 & 更平稳 \\ \hline
收敛速度 & $O(1/k)$ & $O(1/k^2)$在凸情形 \\ \hline
\end{tabular}
\caption{Heavy-Ball 与 Nesterov 对比}
\end{table}

\subsection{从 SGD 到动量方法的逻辑链}

SGD $\to$ 抖动严重 \\
$\to$ 引入“惯性”平滑更新 (Heavy-Ball) \\
$\to$ 进一步在“预测点”计算梯度 (Nesterov) \\
$\to$ 演化出现代动量优化器（如 Adam, RMSProp, AdaBelief）中融合动量思想的分支。

\section{AdaGrad（Duchi et al., 2011）}

\subsection{动机：SGD 学习率“一刀切”的问题}

SGD 使用固定学习率：
\[
x_{t+1} = x_t - \eta \nabla f_t(x_t)
\]

\textbf{缺陷：}
\begin{enumerate}
    \item 各参数维度梯度尺度不同，统一学习率不合理。
    \item 稀疏特征学习慢（小梯度参数被忽略）。
    \item 学习率难以手动调整。
\end{enumerate}

\textbf{核心想法：} \\
让每个参数拥有\textbf{独立的、自适应的学习率}。 \\
梯度大的维度 $\to$ 下降步长变小； \\
梯度小的维度 $\to$ 下降步长变大。

\subsection{算法公式}

\begin{algorithm}[AdaGrad 算法]
设第 $i$ 个参数的历史梯度平方和为：
\[
G_{t,i} = \sum_{\tau=1}^{t} (\nabla_{i} f_\tau(x_\tau))^2
\]

更新规则为：
\[
x_{t+1,i} = x_{t,i} - \frac{\eta}{\sqrt{G_{t,i}} + \varepsilon} \nabla_{i} f_t(x_t)
\]

或向量形式：
\[
x_{t+1} = x_t - \eta \cdot D_t^{-1/2} \nabla f_t(x_t)
\]
其中：
\[
D_t = \mathrm{diag}(G_t) + \varepsilon I
\]
\end{algorithm}

\subsection{性质与效果}

\begin{enumerate}
    \item \textbf{方向自适应}：梯度大（噪声多）的维度衰减快，梯度小的维度保持较大学习率。
    \item \textbf{天然适用于稀疏数据}（如 NLP 中的 embedding 训练），罕见词梯度少 $\to$ 步长较大。
    \item \textbf{单调递减学习率}： \\
    因为 $G_{t,i}$ 累积增长，$\sqrt{G_{t,i}}$ 也持续增大。 \\
    结果是学习率不断衰减，最终趋近于 0。
\end{enumerate}

\subsection{优缺点}

\textbf{优点：}
\begin{itemize}
    \item 不需要手动调节学习率；
    \item 稀疏特征训练效果突出；
    \item 理论上可证明收敛率 $O(1/\sqrt{t})$。
\end{itemize}

\textbf{缺点：}
\begin{itemize}
    \item 学习率衰减过快（在非凸问题上几乎停止更新）；
    \item 对密集梯度任务表现差（如深度网络）。
\end{itemize}

\subsection{本质理解：累积“几何尺度”的归一化}

从几何角度看，AdaGrad 在梯度空间中进行\textbf{各向异性缩放}（anisotropic scaling）：
\[
\Delta x_t = -\eta (G_t)^{-1/2} \nabla f_t(x_t)
\]
即在每个方向上使用“与历史梯度能量成反比”的缩放。 \\
可以理解为在一个逐步扭曲的\textbf{黎曼度量（Riemannian metric）}下优化。

\begin{quote}
AdaGrad 相当于在每一步都重新定义“距离”的概念，让常被更新的方向走得更谨慎，少被更新的方向走得更大胆。
\end{quote}

\section{RMSProp}

\subsection{动机：修正 AdaGrad 的“学习率枯竭”}

AdaGrad 的问题核心在于
\[ G_{t,i} = \sum_{\tau=1}^{t} (\nabla_{i} f_\tau)^2 \]
不断累加，使得分母
\[ \sqrt{G_{t,i}} \]
持续增大 $\to$ 学习率单调下降 $\to$ 在训练后期几乎不动。

\textbf{RMSProp 的核心改进：}
不再无限累积，而是使用\textbf{指数滑动平均（EMA）}仅保留“近期”梯度信息。

\subsection{算法公式}

\begin{algorithm}[RMSProp 算法]
定义平方梯度的滑动平均：
\[
 E[g^2]_t = \rho E[g^2]_{t-1} + (1-\rho) (\nabla f_t(x_t))^2
\]
其中 $\rho \in [0,1)$ 通常取 0.9。

更新规则：
\[
 x_{t+1} = x_t - \frac{\eta}{\sqrt{E[g^2]_t + \varepsilon}} \odot \nabla f_t(x_t)
\]

或分量形式：
\[
 x_{t+1,i} = x_{t,i} - \frac{\eta}{\sqrt{E[g_i^2]_t + \varepsilon}} \cdot \nabla_{i} f_t(x_t)
\]
\end{algorithm}

\subsection{性质与直觉}

\begin{enumerate}
    \item \textbf{滑动窗口的能量归一化：}
    不再记住所有历史，而只记住“近几步”的梯度能量。
    这样学习率不会无限衰减。
    \item \textbf{自适应但平稳：}
    对于方差大的维度（梯度震荡），分母变大 $\to$ 学习率下降；
    对于稳定的维度，学习率保持相对较大。
    \item \textbf{鲁棒性好}：
    对非平稳损失（如深度网络早期阶段的抖动）有缓冲作用。
\end{enumerate}

\subsection{与 AdaGrad 的对比}

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{特性} & \textbf{AdaGrad} & \textbf{RMSProp} \\ \hline
梯度记忆 & 全历史累积 & 指数滑动平均 \\ \hline
学习率 & 单调递减至零 & 稳定在某范围 \\ \hline
稀疏特征适配 & 强 & 一般 \\ \hline
深度网络表现 & 弱 & 强 \\ \hline
\end{tabular}
\caption{AdaGrad 与 RMSProp 对比}
\end{table}

\subsection{本质理解}

RMSProp 相当于让优化器在一个“动态调整的、局部平滑”的度量空间中前进。
在几何意义上，它不是单调放大的尺度，而是根据当前梯度方差\textbf{实时自适应地拉伸或压缩}参数空间。

换句话说：

\begin{quote}
AdaGrad 是“记仇”的学生（过去的错误全记着）； \\
RMSProp 是“善忘”的学生（只记得最近的错误）。
\end{quote}

\section{Adam（Adaptive Moment Estimation）}

\subsection{动机：融合动量与自适应学习率}

前两者的优劣：
\begin{itemize}
    \item \textbf{动量法}（Heavy-Ball / Nesterov）平滑方向，加速收敛。
    \item \textbf{RMSProp} 通过平方梯度的滑动平均调节学习率，抑制震荡。
\end{itemize}

Adam 结合两者：

\begin{quote}
“动量提供方向一致性，RMSProp提供步长自适应。”
\end{quote}

同时引入\textbf{偏置校正}（bias correction）解决初始化期估计偏小的问题。

\subsection{算法核心公式}

\begin{algorithm}[Adam 算法]
定义梯度：
\[
 g_t = \nabla f_t(x_t)
\]

\begin{enumerate}
    \item \textbf{一阶动量（梯度均值）}
    \[
    m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t
    \]

    \item \textbf{二阶动量（平方梯度均值）}
    \[
    v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
    \]

    \item \textbf{偏置校正（bias correction）}
    在初期 $m_t, v_t$ 向 0 偏移，故修正为：
    \[
    \hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad
    \hat{v}_t = \frac{v_t}{1 - \beta_2^t}
    \]

    \item \textbf{更新规则：}
    \[
    x_{t+1} = x_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \varepsilon}
    \]
\end{enumerate}
\end{algorithm}

\subsection{性质与优点}

\begin{enumerate}
    \item \textbf{方向平滑 + 步长自适应：}
    \begin{itemize}
        \item $m_t$ 平滑梯度方向，避免震荡；
        \item $v_t$ 控制各维度学习率，防止陡峭方向过冲。
    \end{itemize}
    \item \textbf{偏置校正保证早期稳定性：}
    使早期梯度统计不再被低估。
    \item \textbf{无需手动调参：}
    在大部分任务中默认参数即可工作良好。
    \item \textbf{适用于非平稳目标、稀疏特征与深度网络。}
\end{enumerate}

\subsection{缺点与改进方向}

\begin{enumerate}
    \item \textbf{可能欠收敛}：
    在部分凸问题中，Adam 不一定收敛到最优点（Reddi et al., 2018 指出）。
    \item \textbf{过度自适应导致步长不稳定}：
    解决方案有 \textbf{AMSGrad}、\textbf{AdamW}、\textbf{AdaBelief} 等。
\end{enumerate}

\subsection{本质理解}

Adam 在几何意义上是\textbf{时间加权的各向异性梯度法}：

\begin{itemize}
    \item $m_t$ 代表\textbf{一阶动量场}，提供“惯性”；
    \item $v_t$ 则定义一个\textbf{时变的度量张量（metric tensor）}，自适应调整每个方向的步长；
    \item 整体行为等价于在动态曲率修正的黎曼空间中作平滑梯度下降。
\end{itemize}

直观比喻：

\begin{quote}
SGD 是盲目走路的人， \\
RMSProp 是谨慎地根据地形调整步伐的人， \\
Adam 是既看惯性又看地形的“自动巡航”行者。
\end{quote}

\section{AdamW（Adam with Decoupled Weight）}

\subsection{动机：修正 Adam 正则化的逻辑错误}

Adam 原版通常通过 \textbf{L2 正则项} 实现权重衰减：
\[
 \min_x f(x) + \frac{\lambda}{2}|x|^2
\]
SGD 中这等价于\textbf{在梯度中加项} $\lambda x$：
\[
 x_{t+1} = x_t - \eta (\nabla f_t(x_t) + \lambda x_t)
\]
但 Adam 的更新包含自适应缩放：
\[
x_{t+1} = x_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \varepsilon}
\]
此时把 $+\lambda x_t$ 加进梯度会被 $(\sqrt{\hat{v}_t}+\varepsilon)^{-1}$ 缩放，
导致正则化强度与梯度统计耦合，\textbf{不再是纯粹的权重衰减}。

\subsection{核心思想：权重衰减与梯度更新解耦}

AdamW 的关键修改：
不再把 $\lambda x_t$ 加入梯度，而是直接对参数施加衰减：
\begin{algorithm}[AdamW 算法]
\[
m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t
\]
\[
v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
\]
\[
\hat{m}_t = \frac{m_t}{1 - \beta_1^t}
\]
\[
\hat{v}_t = \frac{v_t}{1 - \beta_2^t}
\]
\[
x_{t+1} = x_t - \eta \left( \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \varepsilon} + \lambda x_t \right)
\]
\end{algorithm}

注意：
衰减项 $\lambda x_t$ 不再乘以自适应比例因子。

\subsection{性质与效果}

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{特性} & \textbf{Adam} & \textbf{AdamW} \\ \hline
权重衰减 & 通过梯度项实现，受自适应缩放影响 & 与梯度更新解耦，恒定衰减率 \\ \hline
正则化一致性 & 不稳定 & 稳定且可控 \\ \hline
理论收敛性 & 弱 & 改善（更接近 SGD 行为） \\ \hline
实践效果 & 对超参数敏感 & 更鲁棒，普遍优于原版 Adam \\ \hline
\end{tabular}
\caption{Adam 与 AdamW 对比}
\end{table}

\subsection{本质理解}

AdamW 将优化器的两个任务分离：

\begin{enumerate}
    \item \textbf{梯度驱动更新}：由 $\hat{m}_t / \sqrt{\hat{v}_t}$ 决定方向与步长。
    \item \textbf{权重衰减}：独立控制参数幅度，起到正则作用。
\end{enumerate}

几何视角：

\begin{itemize}
    \item Adam 在一个\textbf{动态加权的度量空间}中做下降；
    \item AdamW 额外加入一个\textbf{欧式长度惩罚}，保持参数范数稳定；
    \item 两者独立，因此正则化强度与自适应缩放无关。
\end{itemize}
