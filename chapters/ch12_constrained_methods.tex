\chapter{约束优化问题解法}

本章系统介绍常见的带约束优化问题求解思想与典型算法，包括\textbf{外点法（惩罚函数法）}、\textbf{增广拉格朗日法（ALM）}以及在可分结构下广泛应用的\textbf{ADMM（交替方向乘子法）}。

\section{外点法}
外点法是\textbf{惩罚函数法}（Penalty Function Method）的核心分支，属于约束优化的\textbf{间接解法}——通过将约束条件转化为目标函数的惩罚项，将原约束问题转化为一系列无约束优化子问题，迭代求解无约束子问题的最优解，使其逐步逼近原约束问题的最优解。其核心特征是：\textbf{迭代点始终位于可行域外部}，通过惩罚项迫使迭代点向可行域边界收敛。

\subsection{核心定义与问题形式}
\begin{definition}[原约束优化问题（标准形式）]
设优化变量 $\boldsymbol{x} \in \mathbb{R}^n$，原问题定义为：
\[
\begin{cases}
\min_{\boldsymbol{x}} & f(\boldsymbol{x}) \quad \text{（目标函数，连续可微）} \\
\text{s.t.} & g_i(\boldsymbol{x}) \leq 0 \quad (i=1,2,\dots,m) \quad \text{（不等式约束）} \\
& h_j(\boldsymbol{x}) = 0 \quad (j=1,2,\dots,p) \quad \text{（等式约束）}
\end{cases}
\]
其中：
\begin{itemize}
    \item 可行域 $\Omega = \{\boldsymbol{x} \in \mathbb{R}^n \mid g_i(\boldsymbol{x}) \leq 0, h_j(\boldsymbol{x}) = 0\}$；
    \item 假设 $\Omega \neq \emptyset$（可行域非空），且原问题存在最优解 $\boldsymbol{x}^*$。
\end{itemize}
\end{definition}

\subsubsection{2. 外点法的核心思想}
\begin{itemize}
    \item 对于\textbf{可行域外部的点}（违反约束的点），通过惩罚项施加“惩罚”，使其目标函数值增大；
    \item 惩罚强度由\textbf{惩罚参数 $\mu > 0$} 控制，且 $\mu$ 随迭代逐步增大（$\mu_k \to +\infty$）；
    \item 当 $\mu$ 足够大时，无约束子问题的最优解会“被迫”靠近可行域，最终收敛到原问题的最优解 $\boldsymbol{x}^*$。
\end{itemize}

\subsection{数学建模：惩罚函数构造}
外点法的核心是设计\textbf{惩罚函数 $P(\boldsymbol{x}, \mu)$}，其通用形式为：
\[
P(\boldsymbol{x}, \mu) = f(\boldsymbol{x}) + \mu \cdot \Phi(\boldsymbol{x})
\]
其中：
\begin{itemize}
    \item $f(\boldsymbol{x})$ 为原目标函数；
    \item $\mu > 0$ 为惩罚参数（迭代中满足 $\mu_{k+1} > \mu_k$，且 $\mu_k \to +\infty$）；
    \item $\Phi(\boldsymbol{x})$ 为\textbf{约束违反度量函数}（非负、连续可微），用于量化点 $\boldsymbol{x}$ 对约束的违反程度，满足：
    \[
    \Phi(\boldsymbol{x}) = 0 \iff \boldsymbol{x} \in \Omega \quad \text{（可行点无惩罚）}
    \]
    \[
    \Phi(\boldsymbol{x}) > 0 \iff \boldsymbol{x} \notin \Omega \quad \text{（不可行点有惩罚，违反越严重惩罚越大）}
    \]
\end{itemize}

\subsubsection{3. 约束违反度量函数的具体形式}
根据约束类型（不等式/等式），$\Phi(\boldsymbol{x})$ 通常分解为两部分：
\[
\Phi(\boldsymbol{x}) = \sum_{i=1}^m \phi(g_i(\boldsymbol{x})) + \sum_{j=1}^p \psi(h_j(\boldsymbol{x}))
\]
其中：

\paragraph{（1）不等式约束惩罚项 $\phi(g_i(\boldsymbol{x}))$}
最常用的是\textbf{二次惩罚}（连续可微，便于无约束优化求解）：
\[
\phi(t) = \max(0, t)^2 = 
\begin{cases}
t^2 & t > 0 \quad \text{（违反约束，施加惩罚）} \\
0 & t \leq 0 \quad \text{（满足约束，无惩罚）}
\end{cases}
\]
\begin{itemize}
    \item 其他形式：一次惩罚 $\phi(t) = \max(0, t)$（不可微，仅用于简单问题）、指数惩罚 $\phi(t) = e^{\alpha t} - 1$（$\alpha > 0$，惩罚增长更快）。
\end{itemize}

\paragraph{（2）等式约束惩罚项 $\psi(h_j(\boldsymbol{x}))$}
等式约束无“满足/违反”的中间状态，直接惩罚偏差：
\[
\psi(t) = t^2 \quad \text{（二次惩罚，最常用）}
\]
\begin{itemize}
    \item 其他形式：$\psi(t) = |t|$（不可微）、$\psi(t) = t^4$（惩罚增长更快）。
\end{itemize}

\subsubsection{4. 完整惩罚函数示例（二次惩罚）}
结合上述形式，二次惩罚的外点法惩罚函数为：
\[
P(\boldsymbol{x}, \mu) = f(\boldsymbol{x}) + \mu \left[ \sum_{i=1}^m \max(0, g_i(\boldsymbol{x}))^2 + \sum_{j=1}^p h_j(\boldsymbol{x})^2 \right]
\]

\subsection{算法流程（严格形式化）}
\begin{algorithm}[外点法算法流程]
\textbf{输入}：
\begin{itemize}
    \item 原问题目标函数 $f(\boldsymbol{x})$、约束 $g_i(\boldsymbol{x})$、$h_j(\boldsymbol{x})$；
    \item 初始参数：初始点 $\boldsymbol{x}_0 \in \mathbb{R}^n$（可在可行域外部）、初始惩罚参数 $\mu_1 > 0$、惩罚参数增长因子 $\beta > 1$（通常取 $10$）、收敛精度 $\epsilon > 0$。
\end{itemize}

\textbf{迭代步骤}：
\begin{enumerate}
    \item \textbf{初始化}：令迭代次数 $k = 1$；
    \item \textbf{构造惩罚函数}：针对当前 $\mu_k$，构造 $P(\boldsymbol{x}, \mu_k)$；
    \item \textbf{求解无约束子问题}：以 $\boldsymbol{x}_{k-1}$ 为初始点，求解 $\min_{\boldsymbol{x}} P(\boldsymbol{x}, \mu_k)$，得到最优解 $\boldsymbol{x}_k$；
    \item \textbf{收敛判断}：若满足以下任一收敛条件，停止迭代，输出 $\boldsymbol{x}_k \approx \boldsymbol{x}^*$：
    \begin{itemize}
        \item 约束违反度量足够小：$\Phi(\boldsymbol{x}_k) < \epsilon$；
        \item 目标函数变化足够小：$\| f(\boldsymbol{x}_k) - f(\boldsymbol{x}_{k-1}) \| < \epsilon$；
        \item 迭代点变化足够小：$\| \boldsymbol{x}_k - \boldsymbol{x}_{k-1} \| < \epsilon$；
    \end{itemize}
    \item \textbf{更新惩罚参数}：令 $\mu_{k+1} = \beta \cdot \mu_k$，$k = k + 1$，返回步骤 2。
\end{enumerate}

\textbf{输出}：
原约束问题的近似最优解 $\boldsymbol{x}_k$。
\end{algorithm}

\begin{remark}[无约束子问题求解]
\begin{center}
\begin{tabular}{|p{0.25\textwidth}|p{0.2\textwidth}|p{0.2\textwidth}|p{0.25\textwidth}|}
\hline
\textbf{场景分类} & \textbf{推荐方法} & \textbf{工具/实现} & \textbf{核心注意事项} \\
\hline
低维 ($n \leq 10$) + 光滑 $P(\boldsymbol{x},\mu)$ & 解析法（求梯度=0解方程组） & 手动推导 & 仅适合简单问题/教学验证 \\
\hline
中高维 ($n \leq 1000$) + 光滑 $P(\boldsymbol{x},\mu)$（工程主流） & 拟牛顿法 (L-BFGS/BFGS), 梯度下降法等等 & Scipy.fmin\_bfgs, Matlab.fminunc & 1. 传前一轮 $\boldsymbol{x}_{k-1}$ 作初始点；2. 配合线搜索；3. 病态时优先 L-BFGS \\
\hline
高维 ($n > 1000$) + 光滑 $P(\boldsymbol{x},\mu)$ & 共轭梯度法 & Scipy.fmin\_cg & 低内存消耗，适合大规模问题 \\
\hline
不可微 $P(\boldsymbol{x},\mu)$（一次惩罚等） & 梯度自由法 (Nelder-Mead) & Scipy.fmin & 收敛慢，仅用于简单非光滑问题 \\
\hline
\end{tabular}
\end{center}
\end{remark}

\begin{remark}[惩罚参数更新方法]
\begin{center}
\begin{tabular}{|p{0.15\textwidth}|p{0.25\textwidth}|p{0.15\textwidth}|p{0.15\textwidth}|p{0.2\textwidth}|}
\hline
\textbf{更新方法} & \textbf{公式/逻辑} & \textbf{优点} & \textbf{缺点} & \textbf{适用场景} \\
\hline
经典倍增法则（基础） & $\mu_{k+1} = \beta\mu_k$ ($\beta=5\sim10$) & 实现最简单，无需额外计算 & 易病态，需调 $\beta$ & 快速验证、简单问题 \\
\hline
自适应更新法（首选） & 阈值触发：$\Phi(\boldsymbol{x}_k)>\eta_k$ 则 $\mu_{k+1}=\beta\mu_k$，否则保持 & 平衡收敛速度与稳定性，鲁棒性强 & 需计算约束违反度量 $\Phi(\boldsymbol{x}_k)$ & 工程实践、中高维问题（优先选） \\
\hline
线性增长法 & $\mu_{k+1} = \mu_k + c$ ($c>0$) & 数值稳定性最好，不易病态 & 收敛慢 & 高维等式约束、易病态问题 \\
\hline
指数增长法 & $\mu_{k+1} = \beta^k\mu_1$ & 收敛极快 & 极易病态 & 约束违反程度下降快的简单问题 \\
\hline
KKT残量法 & 按 KKT 残量 $r_k$ 动态调整 $\mu$ & 精准，收敛速率高 & 复杂，需估计拉格朗日乘子 & 高精度需求、学术研究 \\
\hline
\end{tabular}
\end{center}
\end{remark}

\subsection{收敛性分析（核心结论）}
\begin{theorem}[外点法收敛性]
外点法的收敛性依赖于惩罚参数 $\mu_k \to +\infty$，关键结论如下（严格证明需用到变分不等式或闭映射理论）：
\begin{enumerate}
    \item \textbf{序列有界性}：若原问题最优解存在，且惩罚函数 $P(\boldsymbol{x}, \mu)$ 是强制函数（当 $\|\boldsymbol{x}\| \to +\infty$ 时 $P(\boldsymbol{x}, \mu) \to +\infty$），则迭代序列 $\{\boldsymbol{x}_k\}$ 有界；
    \item \textbf{收敛性}：设 $\{\boldsymbol{x}_k\}$ 是迭代序列，其任一聚点 $\boldsymbol{x}^*$ 都是原约束问题的最优解；
    \item \textbf{收敛速率}：二次惩罚外点法的收敛速率为 \textbf{线性收敛}（当 $\mu_k$ 按指数增长时，可达到超线性收敛）。
\end{enumerate}
\end{theorem}

\paragraph{关键直观解释}
当 $\mu_k$ 增大时，惩罚项权重越来越大：
\begin{itemize}
    \item 若 $\boldsymbol{x}_k$ 仍在可行域外部，惩罚项会主导 $P(\boldsymbol{x}, \mu_k)$，迫使 $\boldsymbol{x}_{k+1}$ 向可行域靠近；
    \item 当 $\mu_k \to +\infty$ 时，可行域外部的点会被赋予无穷大惩罚，因此无约束子问题的最优解必须“落在”可行域边界或内部，即收敛到原问题最优解。
\end{itemize}

\subsection{优缺点}
\subsubsection{优点}
\begin{enumerate}
    \item \textbf{初始点灵活}：无需初始点在可行域内（内点法必须初始点可行），尤其适合可行域难以构造初始点的问题；
    \item \textbf{构造简单}：惩罚函数形式直观，无约束子问题可直接用梯度下降、牛顿法等成熟算法求解；
    \item \textbf{兼容性强}：可同时处理不等式约束和等式约束，无需单独设计逻辑。
\end{enumerate}

\subsubsection{缺点}
\begin{enumerate}
    \item \textbf{惩罚病（Penalty Ill-Conditioning）}：当 $\mu_k$ 过大时，惩罚函数 $P(\boldsymbol{x}, \mu_k)$ 的 Hessian 矩阵会呈现“病态”（条件数极大），导致无约束子问题求解困难（梯度下降步长过小、收敛变慢）；
    \item \textbf{仅收敛到可行域边界}：对于不等式约束，最优解若在可行域内部（内点），外点法仍会收敛到边界（需结合其他准则修正）；
    \item \textbf{线性收敛速率}：相比内点法（超线性收敛），收敛速度较慢，适合中小规模约束优化问题。
\end{enumerate}

\subsubsection{深入分析：$\mu_k$ 过大会让 Hessian 病态？}
\begin{enumerate}
    \item \textbf{Gauss-Newton 近似下的 Hessian 结构}：
    在 Gauss-Newton 近似（忽略约束的二阶项）下，罚函数的 Hessian 可以拆成两部分：
    \[
    H_{\mu_k} := \nabla^2 \Phi_{\mu_k}(x) \approx \underbrace{\nabla^2 f(x)}_{H_f} + \mu_k \underbrace{J_h(x)^T J_h(x)}_{H_h}
    \]
    通俗解释：
    \begin{itemize}
        \item $H_f$ 是\textbf{原目标函数的 Hessian}：反映原目标在当前点的“曲率”；
        \item $H_h$ 是\textbf{等式约束惩罚项的 Hessian}：由约束的雅可比矩阵外积得到，反映约束对惩罚项的“影响强度”；
        \item $\mu_k$ 是惩罚参数：控制约束惩罚项的权重。
    \end{itemize}

    \item \textbf{特征值的变化（矩阵“伸缩能力”的改变）}：
    当 $\mu_k$ 很大时，$H_{\mu_k}$ 的特征值会出现“两极分化”：
    \[
    \lambda_{\text{max}}(H_{\mu_k}) \approx \lambda_{\text{max}}(H_f) + \mu_k \sigma_{\text{max}}^2 \quad (\text{最大特征值被} \mu_k \text{放大})
    \]
    \[
    \lambda_{\text{min}}(H_{\mu_k}) \approx \lambda_{\text{min}}(H_f) \quad (\text{最小特征值基本不变})
    \]

    \item \textbf{条件数暴增 $\to$ Hessian 病态}：
    矩阵的“条件数”是 \textbf{最大特征值 $\div$ 最小特征值}，用来衡量矩阵的“病态程度”：
    \[
    \kappa(H_{\mu_k}) = \frac{\lambda_{\text{max}}(H_{\mu_k})}{\lambda_{\text{min}}(H_{\mu_k})} \approx \frac{\lambda_{\text{max}}(H_f) + \mu_k \sigma_{\text{max}}^2}{\lambda_{\text{min}}(H_f)}
    \]
    当 $\mu_k \to +\infty$ 时，条件数 $\kappa(H_{\mu_k}) \to +\infty$ —— 这就是“Hessian 病态”。

    \item \textbf{病态的后果}：
    \begin{itemize}
        \item \textbf{线性系统求解困难}：迭代法收敛慢，直接法数值不稳定。
        \item \textbf{线搜索步长受限}：病态 Hessian 对应的二次模型会变成“很尖的山谷”，导致线搜索只能小步慢挪。
    \end{itemize}

    \item \textbf{结论}：不要一开始把 $\mu_k$ 设太大，应逐步增大。
\end{enumerate}

\begin{remarkinner}
\subsubsection{深入分析：等式约束惩罚项的本质}

等式二次罚函数形式 $ \frac{\mu_k}{2}\|h(x)\|^2 = \frac{\mu_k}{2}\sum_i h_i(x)^2 $，是等式约束下二次罚函数的\textbf{标准形式}（乘以$\frac{1}{2}$是为了推导方便）。

对罚函数 $ \frac{\mu_k}{2}\|h(x)\|^2 $ 求梯度：
根据链式法则，单个 $ h_i(x)^2 $ 的梯度是 $ 2h_i(x)\nabla h_i(x) $，汇总后为 $ \mu_k\sum_i h_i(x)\nabla h_i(x) $。
而雅可比矩阵 $ J_h(x) $ 的定义是“每行对应 $ \nabla h_i(x)^T $”，因此 $ J_h(x)^T $ 的每列对应 $ \nabla h_i(x) $，乘以 $ h(x) $（列向量）恰好得到 $ \sum_i h_i(x)\nabla h_i(x) $。
最终梯度 $ \nabla\left( \frac{\mu_k}{2}\|h\|^2 \right) = \mu_k J_h(x)^T h(x) $。

对梯度 $ \mu_k J_h(x)^T h(x) $ 求 Hessian（二阶导数），需用\textbf{乘积求导法则}：
\[
\nabla^2\left( \mu_k J_h^T h \right) = \mu_k \left( \nabla(J_h^T) \cdot h + J_h^T \cdot \nabla h \right)
\]
\begin{itemize}
    \item 第一项 $ \nabla(J_h^T) \cdot h $ 对应“雅可比矩阵的导数乘以 $ h $”，即 $ \sum_i h_i(x)\nabla^2 h_i(x) $（因为 $ J_h $ 的元素是 $ \partial h_i/\partial x_j $，其导数是 $ \partial^2 h_i/\partial x_j\partial x_k $，即 $ \nabla^2 h_i $ 的元素）；
    \item 第二项 $ J_h^T \cdot \nabla h $ 对应“$ J_h^T $ 乘以 $ h $ 的雅可比（即 $ J_h $）”，即 $ J_h^T J_h $。
\end{itemize}

因此 Hessian $ \nabla^2\left( \frac{\mu_k}{2}\|h\|^2 \right) = \mu_k\left( J_h^T J_h + \sum_i h_i \nabla^2 h_i \right) $。

当迭代点处于\textbf{可行邻域}（$ h(x) \approx 0 $）时，$ \sum_i h_i \nabla^2 h_i \approx 0 $；
而\textbf{Gauss-Newton 近似}本身就是“忽略残差项的二阶导数（即 $ \sum_i h_i \nabla^2 h_i $）”，因此简化后得到 $ \nabla^2 \Phi_{\mu_k(x) }\approx \nabla^2 f(x) + \mu_k J_h^T J_h $，是合理的近似（工程中广泛使用）。

\begin{itemize}
    \item $ \mu_k J_h^T J_h $ 是\textbf{正半定矩阵}：对任意向量 $ d $，有 $ d^T (\mu_k J_h^T J_h) d = \mu_k \|J_h d\|^2 \geq 0 $，符合“正则项需正半定”的要求；
    \item 特征值与方向的关系：
    \begin{itemize}
        \item 沿\textbf{等式法向}（$ J_h $ 的行空间，即不可行方向）：$ J_h d \neq 0 $，正则项贡献 $ \mu_k \|J_h d\|^2 $，特征值随 $ \mu_k $ 线性增大，实现“强回拉”；
        \item 沿\textbf{切空间}（$ J_h d = 0 $，即可行方向）：正则项贡献为 0，由原目标函数的 Hessian $ \nabla^2 f $ 主导，不干扰可行方向的搜索。
    \end{itemize}
\end{itemize}

\subsubsection{深入分析：$(g(x)^+)^2$ 只惩罚越界方向}

\paragraph{一维标量情形}
对于一维变量 $t$，定义惩罚函数：
\[
r(t) = (t_+)^2 = 
\begin{cases} 
t^2, & t > 0 \\
0, & t \leq 0 
\end{cases}
\]
其中 $t_+ = \max(0, t)$，表示只取 $t$ 的非负部分（即“越界”部分）。

\paragraph{导数分析（一阶性质）}
该惩罚函数的导数为：
\[
r'(t) = 
\begin{cases} 
0, & t \leq 0 \\
2t, & t > 0 
\end{cases}
\]
\textbf{关键性质}：
在 $t=0$ 处，左导数（$t \to 0^-$）为 0，右导数（$t \to 0^+$）也为 0，因此 $r(t)$ 在 $t=0$ 处\textbf{可导}（光滑过渡）。

\textbf{直观理解}：
\begin{itemize}
    \item 当 $t \leq 0$（未越界）时，导数为 0，惩罚函数无变化趋势，即“不惩罚”；
    \item 当 $t > 0$（越界）时，导数为 $2t$（随越界程度增大而增大），惩罚函数随 $t$ 增大而快速增长，即“只惩罚越界方向”。
\end{itemize}

\paragraph{推广到多维不等式约束分量 $g_j(x)$}
对于优化问题中的第 $j$ 个不等式约束 $g_j(x) \leq 0$（可行域要求 $g_j(x) \leq 0$，越界即 $g_j(x) > 0$），其惩罚项 $(g_j(x)^+)^2$ 的梯度为：
\[
\nabla (g_j(x)^+)^2 = 2g_j(x)^+ \nabla g_j(x) = 
\begin{cases} 
0, & g_j(x) \leq 0 \\
2g_j(x) \nabla g_j(x), & g_j(x) > 0 
\end{cases}
\]
\textbf{核心结论}：
\begin{itemize}
    \item 当约束满足时（$g_j(x) \leq 0$），梯度为 0，惩罚项对优化方向无影响（不惩罚）；
    \item 当约束被违反时（$g_j(x) > 0$），梯度非零（与 $\nabla g_j(x)$ 同向），推动迭代点向 $g_j(x)$ 减小的方向移动（即向可行域回归，只惩罚越界方向）。
\end{itemize}
\end{remarkinner}
\paragraph{Hessian 矩阵分析（二阶性质）}
惩罚项 $(g_j(x)^+)^2$ 的二阶导数（Hessian 矩阵）为：
\[
\nabla^2 (g_j(x)^+)^2 = 2\nabla g_j(x) \nabla g_j(x)^T + 2g_j(x) \nabla^2 g_j(x)
\]
\textbf{可行边界附近的近似}：
当迭代点接近可行边界（$g_j(x) \approx 0^+$，即刚越界时），$g_j(x) \approx 0$，第二项可忽略，因此 Hessian 近似为：
\[
\nabla^2 (g_j(x)^+)^2 \approx 2\nabla g_j(x) \nabla g_j(x)^T
\]
\textbf{意义}：
近似后的 Hessian 是半正定矩阵（外积形式），其“增强方向”与约束的梯度 $\nabla g_j(x)$ 一致（即越界方向）。这意味着在可行边界附近，惩罚项的二阶特性会“抬升”越界方向的曲率，进一步阻止迭代点向越界方向移动，强化对越界方向的惩罚。

\paragraph{总结}
$(g_j(x)^+)^2$ 作为不等式约束的惩罚项，通过一阶导数（梯度）和二阶导数（Hessian）的设计，实现了“只在约束被违反时生效”的特性：
\begin{itemize}
    \item 可行域内（$g_j(x) \leq 0$）：惩罚项及其导数均为 0，不干扰优化；
    \item 可行域外（$g_j(x) > 0$）：惩罚项随越界程度增大而增长，梯度和 Hessian 引导迭代向可行域回归，精准惩罚越界方向。
\end{itemize}
这种特性使其成为不等式约束外点法中最常用的惩罚形式（连续可微且惩罚针对性强）。

\subsection{示例：外点法求解}
我们用\textbf{外点法}求解该约束优化问题，步骤如下：

\subsubsection{一、问题定义}
目标函数：$f(x) = x_1^2 + 2x_2^2 - 2x_1 - 2x_2$
约束：
\begin{itemize}
    \item 等式约束：$h(x) = x_1 + x_2 - 1 = 0$
    \item 不等式约束：$g(x) = x_1 - 0.6 \leq 0$
\end{itemize}

\subsubsection{二、外点法惩罚函数构造}
外点法通过\textbf{惩罚项}将约束转化为无约束问题：
\begin{itemize}
    \item 等式约束用二次惩罚：$h(x)^2$
    \item 不等式约束仅惩罚越界部分：$(\max(0, g(x)))^2$
\end{itemize}
因此，惩罚函数为：
\[
P(x, \mu_k) = f(x) + \mu_k \left[ h(x)^2 + (\max(0, x_1 - 0.6))^2 \right]
\]
其中 $\mu_k > 0$ 是惩罚参数，需逐步增大（$\mu_k \to +\infty$）。

\subsubsection{三、求解无约束子问题（对固定 $\mu_k$）}
对 $P(x, \mu_k)$ 求偏导并令其为 0，分\textbf{不等式约束越界（$x_1 > 0.6$）}和\textbf{不越界（$x_1 \leq 0.6$）}两种情况：

\paragraph{情况 1：$x_1 \leq 0.6$（惩罚项中 $\max(0, x_1-0.6)=0$）}
惩罚函数简化为：
\[
P(x, \mu_k) = x_1^2 + 2x_2^2 - 2x_1 - 2x_2 + \mu_k (x_1 + x_2 - 1)^2
\]
求偏导并令其为 0：
\[
\begin{cases}
\frac{\partial P}{\partial x_1} = 2x_1 - 2 + 2\mu_k(x_1 + x_2 - 1) = 0 \\
\frac{\partial P}{\partial x_2} = 4x_2 - 2 + 2\mu_k(x_1 + x_2 - 1) = 0
\end{cases}
\]
两式相减得 $x_1 = 2x_2$，代入后解得：
\[
x_1 = \frac{2(1+\mu_k)}{2+3\mu_k}, \quad x_2 = \frac{1+\mu_k}{2+3\mu_k}
\]
但当 $\mu_k > 0$ 时，$x_1 = \frac{2(1+\mu_k)}{2+3\mu_k} > 0.6$（验证：$2(1+\mu_k) > 0.6(2+3\mu_k)$ 恒成立），因此\textbf{情况 1 不成立}。

\paragraph{情况 2：$x_1 > 0.6$（惩罚项中 $\max(0, x_1-0.6)=x_1-0.6$）}
惩罚函数为：
\[
P(x, \mu_k) = x_1^2 + 2x_2^2 - 2x_1 - 2x_2 + \mu_k \left[ (x_1 + x_2 - 1)^2 + (x_1 - 0.6)^2 \right]
\]
求偏导并令其为 0：
\[
\begin{cases}
\frac{\partial P}{\partial x_1} = 2x_1 - 2 + 2\mu_k(x_1 + x_2 - 1) + 2\mu_k(x_1 - 0.6) = 0 \\
\frac{\partial P}{\partial x_2} = 4x_2 - 2 + 2\mu_k(x_1 + x_2 - 1) = 0
\end{cases}
\]
两式相减消去 $2\mu_k(x_1 + x_2 - 1)$，得：
\[
x_1(1+\mu_k) - 2x_2 = 0.6\mu_k \implies x_2 = \frac{x_1(1+\mu_k) - 0.6\mu_k}{2}
\]
代入偏导方程，最终解得：
\[
x_1 = \frac{0.6\mu_k^2 + 3.2\mu_k + 2}{\mu_k^2 + 5\mu_k + 2}, \quad x_2 = \frac{0.4\mu_k^2 + 2\mu_k + 1}{\mu_k^2 + 5\mu_k + 2}
\]

\subsubsection{四、令 $\mu_k \to +\infty$，求极限解}
当 $\mu_k$ 足够大时，分子分母的高次项主导，因此：
\[
\lim_{\mu_k \to +\infty} x_1 = \frac{0.6\mu_k^2}{\mu_k^2} = 0.6, \quad \lim_{\mu_k \to +\infty} x_2 = \frac{0.4\mu_k^2}{\mu_k^2} = 0.4
\]

\subsubsection{五、验证最优解}
\begin{itemize}
    \item 约束满足：$x_1 + x_2 = 0.6 + 0.4 = 1$（等式约束），$x_1 - 0.6 = 0$（不等式约束）。
    \item 目标函数值：$f(0.6, 0.4) = 0.6^2 + 2 \times 0.4^2 - 2 \times 0.6 - 2 \times 0.4 = -1.32$。
\end{itemize}
\textbf{最终结果}：原问题的最优解为 $\boldsymbol{x}^* = (0.6, 0.4)$，最优目标函数值为 $f(\boldsymbol{x}^*) = -1.32$。

\subsection{外点法的极限满足 KKT}
\subsubsection{1. 核心定理/引理（基于外点法的推导）}
先将惩罚函数中的 $\frac{\rho}{2}$ 替换为 $\mu_k$，对应惩罚函数为：
\[
\Phi_{\mu_k}(x) = f(x) + \mu_k \|h(x)\|^2 + \mu_k \|g(x)^+\|^2 \quad (g(x)^+ \text{是} g \text{的非负部分})
\]

\begin{lemma}[可行性残差必趋零]
外点法的迭代点 $x^{(k)}$ 满足 $h(x^{(k)}) \to 0$、$g(x^{(k)})^+ \to 0$。
即迭代点会逐渐趋近原问题的可行域（约束违反程度趋近于 0）。
\end{lemma}

\begin{lemma}[构造候选乘子与近似平稳]
定义候选乘子 $v^{(k)} = \mu_k h(x^{(k)})$、$\lambda_j^{(k)} = \mu_k g_j(x^{(k)})^+$（$\lambda^{(k)} \geq 0$），则迭代点满足“近似平稳条件”：
\[
\nabla f(x^{(k)}) + J_h(x^{(k)})^T v^{(k)} + \sum_{j=1}^p \lambda_j^{(k)} \nabla g_j(x^{(k)}) = r^{(k)}
\]
其中 $r^{(k)} \to 0$（近似误差趋近于 0）。
\end{lemma}

\begin{lemma}[MFCQ 推出乘子有界]
若 MFCQ（约束规范）成立，则候选乘子 $v^{(k)}$、$\lambda^{(k)}$ 是有界的；且非活跃约束（$g_j(x^*) < 0$）对应的 $\lambda_j^{(k)} \to 0$。
\end{lemma}

\begin{theorem}[外点法的极限满足 KKT]
外点法的迭代点极限 $x^*$ 是原问题的 KKT 点。
\end{theorem}

\subsubsection{2. 为什么能证明外点法极限满足 KKT？}
通过三个引理逐步“补全” KKT 条件：
\begin{enumerate}
    \item 引理 1 保证\textbf{极限点满足约束}：$h(x^*) = 0$、$g(x^*) \leq 0$；
    \item 引理 2 构造了候选乘子，得到\textbf{近似平稳条件}（接近 KKT 的梯度条件）；
    \item 引理 3 通过 MFCQ 让乘子有界，从而乘子存在收敛子列；
    \item 对近似平稳条件取极限（$r^{(k)} \to 0$、乘子收敛），最终得到完整的 KKT 条件。
\end{enumerate}

\subsubsection{3. 外点法极限满足的 KKT 条件描述}
外点法的迭代点极限 $x^*$ 满足以下 KKT 条件：
\begin{enumerate}
    \item \textbf{约束条件}：$h(x^*) = 0$，$g(x^*) \leq 0$（处于可行域内）；
    \item \textbf{梯度平稳条件}：存在乘子 $v^*$、$\lambda^* \geq 0$，使得：
    \[
    \nabla f(x^*) + J_h(x^*)^T v^* + J_g(x^*)^T \lambda^* = 0
    \]
    （$J_h, J_g$ 是 $h, g$ 的雅可比矩阵）；
    \item \textbf{互补松弛条件}：对每个约束 $j$，$\lambda_j^* g_j(x^*) = 0$（非活跃约束的乘子为 0）。
\end{enumerate}

\section{增广拉格朗日（ALM）}
\subsection{从外点法到增广拉格朗日法（ALM）}
外点法的核心缺陷是 \textbf{惩罚参数 $\mu \to +\infty$ 导致的数值病态}。
增广拉格朗日法（Augmented Lagrangian Method, ALM）的核心改进的是：\textbf{引入拉格朗日乘子近似反映约束的“优先级”，将“单纯增大惩罚”改为“乘子调整+适度惩罚”}，使得惩罚参数 $\mu$ 无需趋于无穷大即可实现收敛，从根本上解决了外点法的数值病态问题。

\subsection{拉格朗日函数的基础铺垫}
原约束优化问题的\textbf{拉格朗日函数}定义为：
\[
\mathcal{L}(\boldsymbol{x}, \boldsymbol{\lambda}, \boldsymbol{\nu}) = f(\boldsymbol{x}) + \sum_{i=1}^m \lambda_i g_i(\boldsymbol{x}) + \sum_{j=1}^p \nu_j h_j(\boldsymbol{x})
\]
其中：
\begin{itemize}
    \item $\boldsymbol{\lambda} = (\lambda_1, \dots, \lambda_m)^T \geq \boldsymbol{0}$（不等式约束的拉格朗日乘子）；
    \item $\boldsymbol{\nu} = (\nu_1, \dots, \nu_p)^T \in \mathbb{R}^p$（等式约束的拉格朗日乘子）。
\end{itemize}
外点法的本质是“忽略乘子，仅用惩罚强制满足约束”，而 ALM 则是“用乘子近似 KKT 条件中的对偶信息，用惩罚修正约束偏差”。

\subsection{增广拉格朗日函数的构造}
ALM 的核心是\textbf{增广拉格朗日函数}，其构造逻辑是：在拉格朗日函数基础上，加入与外点法一致的\textbf{二次惩罚项}。

\subsubsection{1. 标准增广拉格朗日函数}
\[
\mathcal{L}_A(\boldsymbol{x}, \boldsymbol{\lambda}, \boldsymbol{\nu}, \mu) = f(\boldsymbol{x}) + \sum_{i=1}^m \left[ \lambda_i g_i(\boldsymbol{x}) + \frac{\mu}{2} \max\left(0, g_i(\boldsymbol{x}) + \frac{\lambda_i}{\mu}\right)^2 \right] + \sum_{j=1}^p \left[ \nu_j h_j(\boldsymbol{x}) + \frac{\mu}{2} h_j(\boldsymbol{x})^2 \right]
\]

\subsubsection{2. 简化形式与物理意义}
通过代数变形，可将不等式约束的惩罚项简化为更直观的形式（利用 $ \max(0, a)^2 = \max(0, a^2 + 2a \cdot 0)^2 $，但核心是保持“违反约束时惩罚生效”）：
$$
\max\left(0, g_i(\boldsymbol{x}) + \frac{\lambda_i}{\mu}\right)^2 = \max\left(0, g_i(\boldsymbol{x})\right)^2 +2 \frac{\lambda_i}{\mu} g_i(\boldsymbol{x}) + \frac{\lambda_i^2}{\mu^2} \quad (\text{仅当} \ g_i(\boldsymbol{x}) > 0 \ 时成立)
$$
代入增广拉格朗日函数后，常数项 $ \sum_{i=1}^m \frac{\lambda_i^2}{2\mu} $ 不影响无约束优化的最优解（对 $ \boldsymbol{x} $ 求导时消失），因此可简化为：
$$
\mathcal{L}_A(\boldsymbol{x}, \boldsymbol{\lambda}, \boldsymbol{\nu}, \mu) = f(\boldsymbol{x}) + \mu \left[ \sum_{i=1}^m \frac{1}{2} \max\left(0, g_i(\boldsymbol{x})\right)^2 + \sum_{j=1}^p \frac{1}{2} h_j(\boldsymbol{x})^2 \right] + \sum_{i=1}^m \lambda_i g_i(\boldsymbol{x}) + \sum_{j=1}^p \nu_j h_j(\boldsymbol{x})
$$
\textbf{核心物理意义}：
\begin{itemize}
    \item 当迭代点 $\boldsymbol{x}$ 违反约束时，惩罚项 $\mu \cdot \Phi(\boldsymbol{x})$ 生效，强制迭代点向可行域靠近；
    \item 拉格朗日乘子 $\boldsymbol{\lambda}, \boldsymbol{\nu}$ 随迭代更新，逐步逼近最优乘子 $\boldsymbol{\lambda}^*, \boldsymbol{\nu}^*$，其作用是“引导”惩罚项的方向。
\end{itemize}

\subsection{乘子更新规则的推导（基于 KKT 条件）}

ALM 的关键是\textbf{乘子迭代更新}，其更新规则源于无约束子问题的最优性条件和 KKT 条件的一致性。

\subsubsection{1. 无约束子问题的最优性条件}
对于固定的 $\boldsymbol{\lambda}_k, \boldsymbol{\nu}_k, \mu_k$，求解无约束子问题：
\[
\boldsymbol{x}_{k+1} = \arg\min_{\boldsymbol{x}} \mathcal{L}_A(\boldsymbol{x}, \boldsymbol{\lambda}_k, \boldsymbol{\nu}_k, \mu_k)
\]
其最优性条件为梯度为零：
\[
\nabla_{\boldsymbol{x}} \mathcal{L}_A(\boldsymbol{x}_{k+1}, \boldsymbol{\lambda}_k, \boldsymbol{\nu}_k, \mu_k) = \boldsymbol{0}
\]

\subsubsection{2. 对不等式约束乘子 $\lambda_i$ 的更新}
展开梯度条件中关于 $g_i(\boldsymbol{x})$ 的项：
\[
\nabla f(\boldsymbol{x}_{k+1}) + \sum_{i=1}^m \left[ \lambda_{k,i} + \mu_k \cdot \max\left(0, g_i(\boldsymbol{x}_{k+1})\right) \right] \nabla g_i(\boldsymbol{x}_{k+1}) + \sum_{j=1}^p \left[ \nu_{k,j} + \mu_k h_j(\boldsymbol{x}_{k+1}) \right] \nabla h_j(\boldsymbol{x}_{k+1}) = \boldsymbol{0}
\]

根据 KKT 条件，原问题最优解 $\boldsymbol{x}^*$ 满足：
\[
\nabla f(\boldsymbol{x}^*) + \sum_{i=1}^m \lambda_i^* \nabla g_i(\boldsymbol{x}^*) + \sum_{j=1}^p \nu_j^* \nabla h_j(\boldsymbol{x}^*) = \boldsymbol{0}
\]
且互补松弛条件 $\lambda_i^* g_i(\boldsymbol{x}^*) = 0$（可行点 $g_i(\boldsymbol{x}^*) \leq 0$，故 $\lambda_i^* > 0 \implies g_i(\boldsymbol{x}^*) = 0$）。

对比两式，为使 $\boldsymbol{\lambda}_k$ 逐步逼近 $\lambda_i^*$，自然得到\textbf{乘子更新规则}：
\[
\lambda_{k+1,i} = \max\left( 0, \lambda_{k,i} + \mu_k \cdot g_i(\boldsymbol{x}_{k+1}) \right)
\]
\begin{itemize}
    \item 当 $\boldsymbol{x}_{k+1}$ 满足约束（$g_i(\boldsymbol{x}_{k+1}) \leq 0$）且 $\lambda_{k,i} + \mu_k g_i(\boldsymbol{x}_{k+1}) \geq 0$ 时，$\lambda_{k+1,i} = \lambda_{k,i} + \mu_k g_i(\boldsymbol{x}_{k+1})$，逐步逼近最优乘子；
    \item 当 $\lambda_{k,i} + \mu_k g_i(\boldsymbol{x}_{k+1}) < 0$ 时，$\lambda_{k+1,i} = 0$，满足 KKT 条件中 $\lambda_i \geq 0$ 的要求。
\end{itemize}

\subsubsection{3. 对等式约束乘子 $\nu_j$ 的更新}
同理，展开梯度条件中关于 $h_j(\boldsymbol{x})$ 的项，由于等式约束无互补松弛的非负性要求，直接得到：
\[
\nu_{k+1,j} = \nu_{k,j} + \mu_k \cdot h_j(\boldsymbol{x}_{k+1})
\]

\subsection{ALM 算法流程}
\begin{algorithm}[ALM 算法流程]
\textbf{输入}：
\begin{itemize}
    \item 原问题目标函数 $f(\boldsymbol{x})$、约束 $g_i(\boldsymbol{x})$、$h_j(\boldsymbol{x})$；
    \item 初始参数：初始点 $\boldsymbol{x}_0$；初始乘子 $\boldsymbol{\lambda}_0 = \boldsymbol{0}$、$\boldsymbol{\nu}_0 = \boldsymbol{0}$；初始惩罚参数 $\mu_1 > 0$（无需过大，通常取 1 或 10）；惩罚参数增长因子 $\beta > 1$；收敛精度 $\epsilon > 0$。
\end{itemize}

\textbf{迭代步骤}：
\begin{enumerate}
    \item \textbf{初始化}：令迭代次数 $k = 1$；
    \item \textbf{构造增广拉格朗日函数}：针对当前 $\boldsymbol{\lambda}_{k-1}, \boldsymbol{\nu}_{k-1}, \mu_k$，构造 $\mathcal{L}_A(\boldsymbol{x}, \boldsymbol{\lambda}_{k-1}, \boldsymbol{\nu}_{k-1}, \mu_k)$；
    \item \textbf{求解无约束子问题}：以 $\boldsymbol{x}_{k-1}$ 为初始点，求解 $\min_{\boldsymbol{x}} \mathcal{L}_A$，得到最优解 $\boldsymbol{x}_k$；
    \item \textbf{更新拉格朗日乘子}：
    \begin{itemize}
        \item 不等式约束乘子：$\lambda_{k,i} = \max\left( 0, \lambda_{k-1,i} + \mu_k \cdot g_i(\boldsymbol{x}_k) \right)$；
        \item 等式约束乘子：$\nu_{k,j} = \nu_{k-1,j} + \mu_k \cdot h_j(\boldsymbol{x}_k)$；
    \end{itemize}
    \item \textbf{收敛判断}：若满足以下所有收敛条件，停止迭代，输出 $\boldsymbol{x}_k \approx \boldsymbol{x}^*$：
    \begin{itemize}
        \item 约束违反度量足够小：$\Phi(\boldsymbol{x}_k) < \epsilon$；
        \item 乘子变化足够小：$\| \boldsymbol{\lambda}_k - \boldsymbol{\lambda}_{k-1} \| < \epsilon$ 且 $\| \boldsymbol{\nu}_k - \boldsymbol{\nu}_{k-1} \| < \epsilon$；
        \item 迭代点变化足够小：$\| \boldsymbol{x}_k - \boldsymbol{x}_{k-1} \| < \epsilon$；
    \end{itemize}
    \item \textbf{更新惩罚参数}：若未收敛，令 $\mu_{k+1} = \beta \cdot \mu_k$（或保持 $\mu_k$ 不变），$k = k + 1$，返回步骤 2。
\end{enumerate}

\textbf{输出}：
原约束问题的近似最优解 $\boldsymbol{x}_k$ 及对应的最优乘子 $\boldsymbol{\lambda}_k, \boldsymbol{\nu}_k$。
\end{algorithm}

\subsection{ALM 与外点法的核心差异（严谨对比）}
\begin{center}
\begin{tabular}{|p{0.15\textwidth}|p{0.35\textwidth}|p{0.35\textwidth}|}
\hline
\textbf{对比维度} & \textbf{外点法 (Penalty Function Method)} & \textbf{增广拉格朗日法 (ALM)} \\
\hline
核心函数 & 惩罚函数 $P(\boldsymbol{x}, \mu) = f(\boldsymbol{x}) + \mu \Phi(\boldsymbol{x})$ & 增广拉格朗日函数 $\mathcal{L}_A(\boldsymbol{x}, \boldsymbol{\lambda}, \boldsymbol{\nu}, \mu)$ \\
\hline
关键参数 & 仅惩罚参数 $\mu$（需 $\mu \to +\infty$） & 惩罚参数 $\mu$（可固定或适度增大）+ 拉格朗日乘子 $\boldsymbol{\lambda}, \boldsymbol{\nu}$ \\
\hline
数值稳定性 & 差（$\mu$ 过大导致 Hessian 条件数恶化） & 好（乘子引导惩罚，$\mu$ 无需无穷大） \\
\hline
收敛速度 & 慢（依赖 $\mu$ 逐步增大，迭代后期收敛迟缓） & 快（乘子自适应调整约束权重，迭代前期即可快速靠近最优解） \\
\hline
最优性条件满足 & 仅满足原始可行性（$\boldsymbol{x}_k \to \Omega$），对偶信息无保障 & 同时逼近原始最优和对偶最优（$\boldsymbol{\lambda}_k \to \boldsymbol{\lambda}^*$，$\boldsymbol{\nu}_k \to \boldsymbol{\nu}^*$） \\
\hline
\end{tabular}
\end{center}

\subsection{收敛性核心结论}
假设原问题满足：$f(\boldsymbol{x})$ 凸、$g_i(\boldsymbol{x})$ 凸、$h_j(\boldsymbol{x})$ 仿射，且 Slater 条件成立（存在严格可行点），则 ALM 的迭代序列 $\{\boldsymbol{x}_k, \boldsymbol{\lambda}_k, \boldsymbol{\nu}_k\}$ 满足：
\begin{enumerate}
    \item $\boldsymbol{x}_k$ 强收敛到原问题的最优解 $\boldsymbol{x}^*$；
    \item $\boldsymbol{\lambda}_k$ 强收敛到最优拉格朗日乘子 $\boldsymbol{\lambda}^*$，$\boldsymbol{\nu}_k$ 强收敛到 $\boldsymbol{\nu}^*$；
    \item 惩罚参数 $\mu$ 可固定为某个常数（无需增大），仍能保证收敛（这是 ALM 相对于外点法的本质优势）。
\end{enumerate}
若原问题非凸，在适当的约束品性下，ALM 仍能保证迭代序列的聚点是原问题的 KKT 点。

\subsection{示例：ALM 应用}
\[
\begin{cases}
\min_{x_1,x_2} & f(x) = x_1^2 + 2x_2^2 - 2x_1 - 2x_2 \\
\text{s.t.} & h(x) = x_1 + x_2 - 1 = 0 \quad (\text{等式约束}) \\
& g(x) = x_1 - 0.6 \leq 0 \quad (\text{不等式约束})
\end{cases}
\]
\begin{itemize}
    \item 初始点：$\boldsymbol{x}_0 = (0, 0)$
    \item 初始乘子：$\lambda_0 = 0$，$\nu_0 = 0$
    \item 初始惩罚参数：$\mu_1 = 1$
    \item 惩罚参数增长因子：$\beta = 2$
\end{itemize}

\subsubsection{第一轮迭代 ($k=1$)}
\paragraph{步骤 1：构造增广拉格朗日函数}
代入 $\lambda_0=0,\ \nu_0=0,\ \mu_1=1$，简化得：
\[
\mathcal{L}_A(\boldsymbol{x}) = x_1^2+2x_2^2-2x_1-2x_2 + \frac{1}{2}\max\left(0, x_1-0.6\right)^2 + \frac{1}{2}(x_1+x_2-1)^2
\]

\paragraph{步骤 2：求解无约束子问题}
对 $x_1, x_2$ 求偏导并令其为 0。若 $x_1 > 0.6$，偏导为：
\[
\begin{cases}
\frac{\partial \mathcal{L}_A}{\partial x_1} = 4x_1 + x_2 - 3.6 = 0 \\
\frac{\partial \mathcal{L}_A}{\partial x_2} = x_1 + 5x_2 - 3 = 0
\end{cases}
\]
解方程组得：
\[
x_1 \approx 0.7895,\ x_2 \approx 0.4421 \quad (\text{满足} \ x_1>0.6)
\]
即第一轮迭代点：$\boldsymbol{x}_1 = (0.7895, 0.4421)$。

\paragraph{步骤 3：更新乘子}
\begin{itemize}
    \item 不等式约束乘子：$\lambda_1 = \max(0, 0 + 1 \cdot (0.7895-0.6)) = 0.1895$
    \item 等式约束乘子：$\nu_1 = 0 + 1 \cdot (0.7895+0.4421-1) = 0.2316$
\end{itemize}

\subsubsection{第二轮迭代 ($k=2$)}
\paragraph{步骤 1：更新惩罚参数 \& 构造增广拉格朗日函数}
$\mu_2 = 2 \times 1 = 2$。代入 $\lambda_1=0.1895,\ \nu_1=0.2316,\ \mu_2=2$，增广拉格朗日函数为：
\[
\mathcal{L}_A(\boldsymbol{x}) = x_1^2+2x_2^2-2x_1-2x_2 + 0.1895(x_1-0.6) + \max\left(0, x_1-0.50525\right)^2 + 0.2316(x_1+x_2-1) + (x_1+x_2-1)^2
\]

\paragraph{步骤 2：求解无约束子问题}
对 $x_1, x_2$ 求偏导并令其为 0，解得：
\[
x_1 \approx 0.6251,\ x_2 \approx 0.4197 \quad (\text{接近} \ x_1=0.6 \text{的约束边界})
\]
即第二轮迭代点：$\boldsymbol{x}_2 = (0.6251, 0.4197)$。

\paragraph{步骤 3：更新乘子}
\begin{itemize}
    \item 不等式约束乘子：$\lambda_2 = \max(0, 0.1895 + 2 \cdot (0.6251-0.6)) = 0.2397$
    \item 等式约束乘子：$\nu_2 = 0.2316 + 2 \cdot (0.6251+0.4197-1) = 0.3212$
\end{itemize}

\subsubsection{迭代结果总结}
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
迭代轮次 & 迭代点 $\boldsymbol{x}_k$ & 不等式乘子 $\lambda_k$ & 等式乘子 $\nu_k$ & 惩罚参数 $\mu_k$ \\
\hline
初始 & $(0, 0)$ & $0$ & $0$ & $1$ \\
\hline
1 & $(0.7895, 0.4421)$ & $0.1895$ & $0.2316$ & $1$ \\
\hline
2 & $(0.6251, 0.4197)$ & $0.2397$ & $0.3212$ & $2$ \\
\hline
\end{tabular}
\end{center}
可以看到：$\boldsymbol{x}_2$ 已靠近不等式约束边界 $x_1=0.6$，等式约束偏差 $h(\boldsymbol{x}_2) \approx 0.0448$ 也显著减小，说明迭代在向可行域收敛。

\section{约束问题的解法之 ADMM}
\subsection{第一步：先锁定 ALM 的“可分问题特例”（ADMM 的适用场景）}
我们从 ALM 能处理的一般约束问题中，挑一个\textbf{目标可分+线性等式约束}的特例（这正是 ADMM 专门解决的问题）：
\[
\begin{cases}
\min_{\boldsymbol{x}, \boldsymbol{z}} & \underbrace{f(\boldsymbol{x})}_{\text{仅依赖}\boldsymbol{x}} + \underbrace{g(\boldsymbol{z})}_{\text{仅依赖}\boldsymbol{z}} \quad \text{（目标可分）} \\
\text{s.t.} & \boldsymbol{A}\boldsymbol{x} + \boldsymbol{B}\boldsymbol{z} = \boldsymbol{c} \quad \text{（线性等式约束）}
\end{cases}
\]

（注：ADMM 只处理这种“目标拆成两个独立部分+线性等式约束”的问题，这是它和 ALM 的第一个区别——ALM 处理更一般的约束）

\subsection{第二步：写出这个特例的 ALM 增广拉格朗日函数}
\[
\mathcal{L}_A^{\text{ALM}}(\boldsymbol{x}, \boldsymbol{z}, \boldsymbol{\nu}, \rho) = f(\boldsymbol{x}) + g(\boldsymbol{z}) + \boldsymbol{\nu}^T(\boldsymbol{A}\boldsymbol{x} + \boldsymbol{B}\boldsymbol{z} - \boldsymbol{c}) + \frac{\rho}{2}\|\boldsymbol{A}\boldsymbol{x} + \boldsymbol{B}\boldsymbol{z} - \boldsymbol{c}\|_2^2
\]
其中：
\begin{itemize}
    \item $\boldsymbol{\nu}$ 是等式约束的拉格朗日乘子；
    \item $\rho > 0$ 是惩罚参数（和 ALM 的 $\mu$ 是同一个东西）。
\end{itemize}

\subsection{第三步：ALM 对这个问题的迭代步骤}
ALM 处理这个问题时，迭代是“\textbf{同时最小化 $\boldsymbol{x},\boldsymbol{z}$，再更新乘子 $\boldsymbol{\nu}$}”：
\begin{enumerate}
    \item 最小化增广拉格朗日：$(\boldsymbol{x}^{k+1}, \boldsymbol{z}^{k+1}) = \arg\min_{\boldsymbol{x},\boldsymbol{z}} \mathcal{L}_A^{\text{ALM}}(\boldsymbol{x}, \boldsymbol{z}, \boldsymbol{\nu}^k, \rho)$
    \item 更新乘子：$\boldsymbol{\nu}^{k+1} = \boldsymbol{\nu}^k + \rho(\boldsymbol{A}\boldsymbol{x}^{k+1} + \boldsymbol{B}\boldsymbol{z}^{k+1} - \boldsymbol{c})$
\end{enumerate}

\subsection{第四步：关键改进——利用“目标可分”拆分最小化步骤}
问题来了：\textbf{同时最小化 $\boldsymbol{x},\boldsymbol{z}$ 可能很难}（比如 $\boldsymbol{x},\boldsymbol{z}$ 维度很大时）。

但我们的目标是“可分的”：$f(\boldsymbol{x})$ 只和 $\boldsymbol{x}$ 有关，$g(\boldsymbol{z})$ 只和 $\boldsymbol{z}$ 有关。所以可以\textbf{交替最小化 $\boldsymbol{x}$ 和 $\boldsymbol{z}$}（先固定 $\boldsymbol{z}$ 求 $\boldsymbol{x}$，再固定 $\boldsymbol{x}$ 求 $\boldsymbol{z}$）——这就是 ADMM 的核心！

\subsubsection{拆分 1：固定 $\boldsymbol{z}^k$，先求 $\boldsymbol{x}^{k+1}$（ADMM 的 $\boldsymbol{x}$-步）}
把 $\boldsymbol{z}=\boldsymbol{z}^k$ 代入增广拉格朗日，此时只有 $\boldsymbol{x}$ 是变量：
\[
\boldsymbol{x}^{k+1} = \arg\min_{\boldsymbol{x}} \left[ f(\boldsymbol{x}) + \boldsymbol{\nu}^{kT}(\boldsymbol{A}\boldsymbol{x} + \boldsymbol{B}\boldsymbol{z}^k - \boldsymbol{c}) + \frac{\rho}{2}\|\boldsymbol{A}\boldsymbol{x} + \boldsymbol{B}\boldsymbol{z}^k - \boldsymbol{c}\|_2^2 \right]
\]

\subsubsection{拆分 2：固定 $\boldsymbol{x}^{k+1}$，再求 $\boldsymbol{z}^{k+1}$（ADMM 的 $\boldsymbol{z}$-步）}
把 $\boldsymbol{x}=\boldsymbol{x}^{k+1}$ 代入增广拉格朗日，此时只有 $\boldsymbol{z}$ 是变量：
\[
\boldsymbol{z}^{k+1} = \arg\min_{\boldsymbol{z}} \left[ g(\boldsymbol{z}) + \boldsymbol{\nu}^{kT}(\boldsymbol{A}\boldsymbol{x}^{k+1} + \boldsymbol{B}\boldsymbol{z} - \boldsymbol{c}) + \frac{\rho}{2}\|\boldsymbol{A}\boldsymbol{x}^{k+1} + \boldsymbol{B}\boldsymbol{z} - \boldsymbol{c}\|_2^2 \right]
\]

\subsection{第五步：简化符号——引入 ADMM 的“对偶残差 \texorpdfstring{$\boldsymbol{u}$}{u}”}
为了让式子更简洁，ADMM 把 ALM 的乘子 $\boldsymbol{\nu}$ 缩放一下：令 $\boldsymbol{u} = \frac{\boldsymbol{\nu}}{\rho}$（$\boldsymbol{u}$ 就是 ADMM 里的“对偶变量”）。

把 $\boldsymbol{\nu} = \rho\boldsymbol{u}$ 代入上面的式子，就能消掉 $\boldsymbol{\nu}$ 的线性项，最终得到 ADMM 的标准步骤：

\subsection{最终：从 ALM 拆分得到的 ADMM 迭代（一一对应）}
\begin{center}
\begin{tabular}{|p{0.45\textwidth}|p{0.45\textwidth}|}
\hline
\textbf{ALM 的步骤（可分特例）} & \textbf{对应 ADMM 的步骤（符号简化后）} \\
\hline
1. 固定 $\boldsymbol{z}^k$，求 $\boldsymbol{x}^{k+1}$ & 1. $\boldsymbol{x}$-步：$\boldsymbol{x}^{k+1} = \arg\min_{\boldsymbol{x}} \left[ f(\boldsymbol{x}) + \frac{\rho}{2}\|\boldsymbol{A}\boldsymbol{x} + \boldsymbol{B}\boldsymbol{z}^k - \boldsymbol{c} + \boldsymbol{u}^k\|_2^2 \right]$ \\
\hline
2. 固定 $\boldsymbol{x}^{k+1}$，求 $\boldsymbol{z}^{k+1}$ & 2. $\boldsymbol{z}$-步：$\boldsymbol{z}^{k+1} = \arg\min_{\boldsymbol{z}} \left[ g(\boldsymbol{z}) + \frac{\rho}{2}\|\boldsymbol{A}\boldsymbol{x}^{k+1} + \boldsymbol{B}\boldsymbol{z} - \boldsymbol{c} + \boldsymbol{u}^k\|_2^2 \right]$ \\
\hline
3. 更新乘子 $\boldsymbol{\nu}^{k+1}$ & 3. $\boldsymbol{u}$-步（对应乘子更新）：$\boldsymbol{u}^{k+1} = \boldsymbol{u}^k + \boldsymbol{A}\boldsymbol{x}^{k+1} + \boldsymbol{B}\boldsymbol{z}^{k+1} - \boldsymbol{c}$ \\
\hline
\end{tabular}
\end{center}

ADMM 就是\textbf{“可分目标+线性等式约束”问题下的 ALM}——它把 ALM“同时最小化所有变量”的步骤，拆成了“交替最小化可分变量块”的简单步骤，同时缩放了乘子符号让式子更简洁。

\subsection{示例（与前述问题关联的改写）}
要对这个问题用 ADMM 迭代，首先得把它转化为 ADMM 的\textbf{标准形式（可分目标+线性等式约束）}。

\subsubsection{步骤 1：转化为 ADMM 标准形式}
原问题：
\[
\begin{cases}
\min_{x_1,x_2} & f(x_1,x_2) = x_1^2 + 2x_2^2 - 2x_1 - 2x_2 \\
\text{s.t.} & x_1 + x_2 = 1 \quad (\text{等式约束}) \\
& x_1 \leq 0.6 \quad (\text{不等式约束})
\end{cases}
\]
\textbf{拆分变量+转化约束}：
\begin{itemize}
    \item 令\textbf{变量块 1}：$x = x_2$（仅依赖 $x_2$），目标项为 $f(x) = 2x^2 - 2x$；
    \item 令\textbf{变量块 2}：$z = x_1$（仅依赖 $x_1$），目标项为 $g(z) = z^2 - 2z + I(z \leq 0.6)$（$I(\cdot)$ 是指示函数）；
    \item 约束转化为线性等式：$z + x = 1$（对应 ADMM 的标准约束 $\boldsymbol{A}z + \boldsymbol{B}x = c$，这里 $\boldsymbol{A}=\boldsymbol{B}=1, c=1$）。
\end{itemize}

\subsubsection{步骤 2：ADMM 的增广拉格朗日函数}
\[
\mathcal{L}_A(x, z, u, \rho) = f(x) + g(z) + \frac{\rho}{2}\left(z + x - 1 + u\right)^2 - \frac{\rho}{2}u^2
\]
其中 $u$ 是对偶变量，$\rho=1$。

\subsubsection{步骤 3：ADMM 迭代流程（3 步循环）}
初始化：$x^0 = 0$，$z^0 = 0$，$u^0 = 0$。

\paragraph{第 1 轮迭代 ($k=0 \to k=1$)}
\begin{enumerate}
    \item \textbf{x-步}：固定 $z^0=0, u^0=0$，最小化 $\mathcal{L}_A$ 关于 $x$：
    \[
    \min_x \ 2x^2 - 2x + \frac{1}{2}\left(0 + x - 1 + 0\right)^2
    \]
    求导并令其为 0：$4x - 2 + (x - 1) = 5x - 3 = 0 \implies x^1 = 0.6$。

    \item \textbf{z-步}：固定 $x^1=0.6, u^0=0$，最小化 $\mathcal{L}_A$ 关于 $z$：
    \[
    \min_z \ \underbrace{z^2 - 2z + I(z \leq 0.6)}_{\text{含约束}} + \frac{1}{2}\left(z + 0.6 - 1 + 0\right)^2
    \]
    无约束解：$2z - 2 + (z - 0.4) = 3z - 2.4 = 0 \implies z=0.8$（违反约束）；
    约束下最优解：$z^1 = 0.6$（约束边界）。

    \item \textbf{u-步}：更新对偶变量：
    \[
    u^1 = u^0 + z^1 + x^1 - 1 = 0 + 0.6 + 0.6 - 1 = 0.2
    \]
\end{enumerate}

\paragraph{第 2 轮迭代 ($k=1 \to k=2$)}
\begin{enumerate}
    \item \textbf{x-步}：固定 $z^1=0.6, u^1=0.2$：
    \[
    \min_x \ 2x^2 - 2x + \frac{1}{2}\left(0.6 + x - 1 + 0.2\right)^2
    \]
    求导得：$4x - 2 + (x - 0.2) = 5x - 2.2 = 0 \implies x^2 = 0.44$。

    \item \textbf{z-步}：固定 $x^2=0.44, u^1=0.2$：
    \[
    \min_z \ z^2 - 2z + I(z \leq 0.6) + \frac{1}{2}\left(z + 0.44 - 1 + 0.2\right)^2
    \]
    无约束解仍违反 $z \leq 0.6$，故 $z^2 = 0.6$。

    \item \textbf{u-步}：
    \[
    u^2 = 0.2 + 0.6 + 0.44 - 1 = 0.24
    \]
\end{enumerate}

\paragraph{第 3 轮迭代 ($k=2 \to k=3$)}
\begin{enumerate}
    \item \textbf{x-步}：固定 $z^2=0.6, u^2=0.24$：
    \[
    \min_x \ 2x^2 - 2x + \frac{1}{2}\left(0.6 + x - 1 + 0.24\right)^2
    \]
    求导得：$5x - 2.16 = 0 \implies x^3 = 0.432$。

    \item \textbf{z-步}：约束下仍取 $z^3 = 0.6$。

    \item \textbf{u-步}：
    \[
    u^3 = 0.24 + 0.6 + 0.432 - 1 = 0.272
    \]
\end{enumerate}

\subsubsection{迭代趋势}
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
迭代轮次 & $x_k=x_2$ & $z_k=x_1$ & 对偶变量 $u_k$ & 约束满足度 $z_k+x_k-1$ \\
\hline
0 & 0 & 0 & 0 & -1 \\
\hline
1 & 0.6 & 0.6 & 0.2 & 0.2 \\
\hline
2 & 0.44 & 0.6 & 0.24 & 0.04 \\
\hline
3 & 0.432 & 0.6 & 0.272 & 0.032 \\
\hline
\end{tabular}
\end{center}
可以看到：$z_k=x_1$ 稳定在约束边界 $0.6$，$x_k=x_2$ 逐渐靠近最优值 $0.4$（原问题最优解为 $x_1=0.6, x_2=0.4$）。

